<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Parallel Computation ¬∑ Sunny documentation</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Sunny documentation logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Sunny documentation</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/fei2_tutorial/">Case Study: FeI<span>$_{2}$</span></a></li><li><a class="tocitem" href="../examples/out_of_equilibrium/">CP<span>$^2$</span> Skyrmion Quench</a></li><li><a class="tocitem" href="../examples/powder_averaging/">Powder averaged CoRh<span>$_2$</span>O<span>$_4$</span></a></li><li><a class="tocitem" href="../examples/fei2_classical/">Structure Factors with Classical Dynamics</a></li><li><a class="tocitem" href="../examples/ising2d/">Classical Ising model</a></li><li><a class="tocitem" href="../examples/one_dim_chain/">Fitting model parameters in a 1D spin-1 ferromagnetic chain</a></li></ul></li><li><a class="tocitem" href="../library/">Library API</a></li><li><a class="tocitem" href="../structure-factor/">Structure Factor Calculations</a></li><li><a class="tocitem" href="../anisotropy/">Single-Ion Anisotropy</a></li><li><a class="tocitem" href="../writevtk/">Volumetric Rendering with ParaView</a></li><li class="is-active"><a class="tocitem" href>Parallel Computation</a><ul class="internal"><li><a class="tocitem" href="#Review-of-the-serial-workflow"><span>Review of the serial workflow</span></a></li><li><a class="tocitem" href="#Multithreading-approach"><span>Multithreading approach</span></a></li></ul></li><li><a class="tocitem" href="../versions/">Version History</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Parallel Computation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Parallel Computation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SunnySuite/Sunny.jl/blob/main/docs/src/parallelism.md" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Parallelizing-Classical-Structure-Factor-Calculations"><a class="docs-heading-anchor" href="#Parallelizing-Classical-Structure-Factor-Calculations">Parallelizing Classical Structure Factor Calculations</a><a id="Parallelizing-Classical-Structure-Factor-Calculations-1"></a><a class="docs-heading-anchor-permalink" href="#Parallelizing-Classical-Structure-Factor-Calculations" title="Permalink"></a></h1><p>Calculating structure factors with classical dynamics is computationally expensive, and Sunny does not currently parallelize these calculations at a fine-grained level. However, Julia provides facilities that allow users to run multiple simulations in parallel with only a little extra effort. We will look at two approaches to doing this: <a href="https://docs.julialang.org/en/v1/manual/multi-threading/">multithreading</a> and Julia&#39;s <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/"><code>Distributed</code></a> package. We&#39;ll present these approaches in a series of code snippets that can be copied and pasted into your preferred Julia development environment.</p><h2 id="Review-of-the-serial-workflow"><a class="docs-heading-anchor" href="#Review-of-the-serial-workflow">Review of the serial workflow</a><a id="Review-of-the-serial-workflow-1"></a><a class="docs-heading-anchor-permalink" href="#Review-of-the-serial-workflow" title="Permalink"></a></h2><p>The serial approach to calculating a structure factor, covered in  <a href="../examples/fei2_classical/#Structure-Factors-with-Classical-Dynamics">Structure Factors with Classical Dynamics</a>, involves thermalizing a spin <code>System</code> and then calling <a href="../library/#Sunny.add_sample!-Tuple{SampledCorrelations, System}"><code>add_sample!</code></a>. <code>add_sample!</code> uses the state of the <code>System</code> as an initial condition for the calculation of a dynamical trajectory. The correlations of the trajectory are calculated and accumulated into a running average of the <span>$ùíÆ(ùê™,œâ)$</span>. This sequence is repeated to generate additional samples.</p><p>To illustrate, we&#39;ll set up a a simple model: a spin-1 antiferromagnet on an FCC crystal. </p><pre><code class="language-julia hljs">using Sunny, GLMakie

function make_system(cryst; J, dims, seed=nothing)
    sys = System(cryst, dims, [SpinInfo(1, S=1, g=2)], :dipole; seed)
    set_exchange!(sys, J, Bond(1,1,[1,0,0]))
    return sys
end

J = 1.0 # meV 
cryst = Sunny.fcc_primitive_crystal()
sys = make_system(cryst; J, dims=(10,10,2))</code></pre><p>To thermalize and generate equilibrium samples, we&#39;ll need a <a href="../library/#Sunny.Langevin"><code>Langevin</code></a> integrator. </p><pre><code class="language-julia hljs">kT = 0.5    # meV
Œît = 0.05/J
integrator = Langevin(Œît; kT, Œª=0.1)</code></pre><p>Finally, call <a href="../library/#Sunny.dynamical_correlations-Union{Tuple{System{N}}, Tuple{N}} where N"><code>dynamical_correlations</code></a> to configure a <code>SampledCorrelations</code>.</p><pre><code class="language-julia hljs">sc = dynamical_correlations(sys; Œît=0.1, nœâ=100, œâmax=10.0)</code></pre><p>The serial calculation can now be performed as follows:</p><pre><code class="language-julia hljs">nsamples = 10

# Thermalize the system
for _ in 1:5000  # Sufficient number of steps to thermalize
    step!(sys, integrator)
end

for _ in 1:nsamples
    # Generate new sample by running Langevin dynamics
    for _ in 1:1000  # Sufficient number of steps to decorrelate
        step!(sys, integrator)
    end
    # Add the sample to the correlation data
    add_sample!(sc, sys)
end</code></pre><p>This will take a second or two on a modern workstation, resulting in a single <code>SampledCorrelations</code> that contains 10 samples.</p><h2 id="Multithreading-approach"><a class="docs-heading-anchor" href="#Multithreading-approach">Multithreading approach</a><a id="Multithreading-approach-1"></a><a class="docs-heading-anchor-permalink" href="#Multithreading-approach" title="Permalink"></a></h2><p>To use threads in Julia, you must launch your Julia environment appropriately.</p><ul><li>From the <strong>command line</strong>, launch Julia with <code>julia --threads=N</code>, where <code>N</code></li></ul><p>is the number of threads. If you don&#39;t know how many threads you&#39;d like, you can let Julia decide with <code>--threads=auto</code>.</p><ul><li><strong>Jupyter notebook</strong> users will need to to set up a multithreaded Julia kernel</li></ul><p>and restart into this kernel. The kernel can be created inside Julia with the following:</p><pre><code class="nohighlight hljs">    using IJulia
    IJulia.installkernel(&quot;Julia Multithreaded&quot;,
        env=Dict(&quot;JULIA_NUM_THREADS&quot; =&gt; &quot;auto&quot;))
    ```

- **VSCode** users should open their settings and search for
`Julia: Additional Args`. There will be link called `Edit in settings.json`. Click on this and add
`&quot;--threads=auto&quot;` to the list `julia.additionalArgs`. Then start a new REPL.

Before going further, make sure that `Threads.nthreads()` returns a number greater than 1.

We will use multithreading in a very simple way, essentially employing a
distributed memory approach to avoid conflicts around memory access. First
preallocate a number of systems and correlations.
</code></pre><p>julia npar = Threads.nthreads() systems = [make<em>system(cryst; J, dims=(10,10,2), seed=i) for i in 1:npar] scs = [dynamical</em>correlations(sys; Œît=0.1, nœâ=100, œâmax=10.0) for _ in 1:npar]</p><pre><code class="nohighlight hljs">
!!! warning &quot;Dealing with memory constraints&quot;

    If you have many threads available and are working with a large system, you
    may not have enough memory to store all these systems and correlations. In
    that case, simply reduce `npar` to a small enough value that you can make the
    necessary allocations.

When the `Threads.@threads` macro is applied before a `for` loop, the
iterations of the loop will execute in parallel using the available threads.
We will put the entire thermalization and sampling process inside the loop,
with each thread acting on a unique `System` and `SampledCorrelations`.
</code></pre><p>julia Threads.@threads for id in 1:npar     integrator = Langevin(Œît; kT, Œª=0.1)     for _ in 1:5000         step!(systems[id], integrator)     end     for _ in 1:nsamples         for _ in 1:1000             step!(systems[id], integrator)         end         add_sample!(scs[id], systems[id])     end end</p><pre><code class="nohighlight hljs">
You may find this takes a little bit longer than the serial example, but, at the
end of it, you will have generated `npar` correlation objects, each with 10
samples. We can merge these into a summary `SampledCorrelations` with `10*npar`
samples with `merge_correlations`:
</code></pre><p>julia sc = merge_correlations(scs)</p><pre><code class="nohighlight hljs">
## Using `Distributed`
Julia also provides a distributed memory approach to parallelism through the
standard library package `Distributed`. This works by launching independent
Julia environments on different &quot;processes.&quot; An advantage of this approach is
that it scales naturally to clusters since the processes are easily distributed
across many different compute nodes. A disadvantage, especially when working on
a single computer, is the increased memory overhead associated with launching
many Julia environments.

We begin by importing the package,
</code></pre><p>julia using Distributed</p><pre><code class="nohighlight hljs">
and launching some new processes. It is often sensible to create as many
processes as there are cores.
</code></pre><p>julia ncores = length(Sys.cpu_info()) addprocs(ncores)</p><pre><code class="nohighlight hljs">
You can think of each process as a separate computer running a fresh Julia
environment, so we&#39;ll need to import Sunny and define our function inside each
of them. This is easily achieved with the `@everywhere` macro.</code></pre><p>julia @everywhere using Sunny</p><p>@everywhere function make<em>system(cryst; J, dims, seed=nothing)     sys = System(cryst, dims, [SpinInfo(1, S=1, g=2)], :dipole; seed)     set</em>exchange!(sys, J, Bond(1,1,[1,0,0]))     return sys end</p><pre><code class="nohighlight hljs">
A simple way to perform work on these processes is to use the parallel map
function, `pmap`. This will apply a function to each element of some iterable,
such as a list of numbers, and return a list of the results. It is a _parallel_
map because these function calls may occur at the same time on different Julia
processes. The `pmap` function takes care of distributing the work among the
different processes and retrieving the results.

In the example below, we give `pmap` a list of RNG seeds to iterate over, and
we define the function that will be applied to each of these seeds in a `do`
block. The contents of this block are essentially the same as what we put
inside our parallel `for` loop in the multithreading example. The main
difference is that the `System`s and `SampledCorrelations` are not created in
advance of the parallelization but are instead created inside each Julia
process. The `do` block returns a `SampledCorrelations`, and the output of all
the parallel computations are collected into list of `SampledCorrelations`
called `scs`.
</code></pre><p>julia scs = pmap(1:ncores) do seed     sys = make<em>system(Sunny.fcc</em>primitive<em>crystal(); J=1.0, dims=(10,10,2), seed)     sc = dynamical</em>correlations(sys; Œît=0.1, nœâ=100, œâmax=10.0)     integrator = Langevin(Œît; kT, Œª=0.1)</p><pre><code class="nohighlight hljs">for _ in 1:5000      # Thermalize
    step!(sys, integrator)
end
for _ in 1:nsamples 
    for _ in 1:1000 
        step!(sys, integrator)
    end
    add_sample!(sc, sys)
end

return sc</code></pre><p>end</p><pre><code class="nohighlight hljs">
Finally, merge the results into a summary `SampledCorrelations`.
</code></pre><p>julia sc = merge_correlations(scs) ```</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../writevtk/">¬´ Volumetric Rendering with ParaView</a><a class="docs-footer-nextpage" href="../versions/">Version History ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Saturday 9 September 2023 17:38">Saturday 9 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
